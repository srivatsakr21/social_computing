{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Computing/Social Gaming - Summer 2020\n",
    "\n",
    "# Exercise Sheet 4: Sentiment Analysis in DotA\n",
    "\n",
    "In this exercise we will work with data gathered from the popular MOBA *Defense of the Ancients 2* or short DotA 2, developed by Valve in 2013. If you are unfamiliar with this game, we provide a short introduction that should be enough to make you understand what the tasks demand from you:\n",
    "\n",
    "In Dota 2, two teams of 5 players play against each other on a single map, each team trying to destroy the enemy base, also called the ancient. In order to do this, they try to kill each other, earn gold and experience by killing non player characters called creeps to gain an advantage over the enemy. In every match, players first choose from a pool of 117 different heroes which are roughly divided into 2 groups: Carries, who start out weak and become much stronger once they accumulated a sufficient amount of gold to buy items they need, and supports, who start protect the carries in the early stages of a match, but tend to become less relevant in the later stages. Every team needs a balanced hero selection in order to have a chance of winning, as too many carries will have that team face a disadvantage early on, while too many supports may cause that team to struggle to win the game even once an advantage has been secured early.\n",
    "\n",
    "Psychologically speaking, DotA - or any MOBA for that matter - is an experiment on succesful team formation and cooperation, as 5 strangers meet each other for one match with the same goal, but usually different views on how to achieve it. Its real world equivalent would be any mash-up of people forced to work in a group, the only difference being that usually real-life situations don't involve another group working against them. \n",
    "\n",
    "Needless to say, the nature of the game does provoke negativity at times, and we want to try to predict it. More precisely, we want to find out whether we can infer negative player behavior from modelling the state of a game as a set of values.\n",
    "\n",
    "The .csv files provided for you contain information from 1.500 matches played during December 2016, and are split into 5 tables: \n",
    "\n",
    "- chat.csv : this table contains information about what was said in the chat between teams, when it was said and which player said it. We need the 'key', 'time' and 'slot' column as we are only interested in which team the players belong to, not their identities.\n",
    "- match.scsv: contains information about the game results. We only need the 'radiant_win' column from it, which tells us which team won.\n",
    "- players.csv : Detailed statistics for every player. 'kills' and 'deaths' columns are needed as we will need them to determine underperforming players\n",
    "- player_times.csv: Among other things the gold accumulated by every player, for every minute of a match, used to calculate the difference in gold earned between these teams.\n",
    "- labels.xlsx : a sample of labeled chat used for the sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.1: Your first sentiment analysis\n",
    "\n",
    "Sentiment analysis, sometimes called opinion mining, is a method to derive information from the text that allows for a classification as neutral, positive or negative. It is a semi-supervised process, meaning that you need a small set of labeled data to train your machine learning model on in order to use it on another set of unlabeled data. Without the labeled set it would be difficult for your AI to know what exactly makes a statement positive or negative. Some of its many practical applications are the analysis of customer reviews, social media comments or survey responses.\n",
    "\n",
    "**Note:** We will use a random forest classifier in this task.\n",
    "\n",
    "Your task is to train a model using the labeled data, then use that model to predict the sentiments of the whole chat. Let us start with the basics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Preparation\n",
    "\n",
    "Import the labels and split them into two arrays: the chat itself and the labels.\n",
    "\n",
    "A label is like a review of a single message:  \n",
    "-1 = negative  \n",
    " 0 = neutral   \n",
    " 1 = positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries needed for sentiment analysis\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re\n",
    "import nltk \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#TODO: Import \"labels.xlsx\" and split it into 2 arrays: chat and labels.\n",
    "\n",
    "\n",
    "chat_data = chat_data[:450]\n",
    "chat_labels = chat_labels[:450]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) How to train your model\n",
    "\n",
    "In this step you will use the chat and labels to train your random forest classifier. In order to do so, create the random forest classifier, fit it and make a prediction on the test set.\n",
    "\n",
    "After you are done, print the accuracy score and comment on it.\n",
    "\n",
    "**Hints:**\n",
    "- When creating the classifier, use n_estimators=200, random_state=0 as arguments.\n",
    "- The test should be 20% of the whole set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There are some words which do not have any valuable classification information. \n",
    "# We will use 'stopwords' to get rid of them.\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "chat_data = [str (item) for item in chat_data]\n",
    "vectorizer = TfidfVectorizer (max_features=2500, min_df=3, max_df=0.8, stop_words=stopwords.words('english'))\n",
    "chat_data = vectorizer.fit_transform(chat_data).toarray()\n",
    "\n",
    "# TODO: Create the random forest classifier, fit it and make a prediction on the test set.\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Write your observations here:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Prediction time:\n",
    "\n",
    "Now you can use the model to predict the sentiments for the whole chat. Import the chat and predict the labels. You will need to use `vectorizer.transform().toarray()` on your data, but **DO NOT** use `fit()` anywhere! The classifier is already fitted, fitting it again effectively erases all it has learned.\n",
    "\n",
    "**Hint:** chat.csv includes ALL chatlines, including those that have been used in the previous steps and are already labeled. Don't label them again.\n",
    "\n",
    "**Note:** The chat table is massive. Labelling all of it may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chatData = pd.read_csv(\"chat.csv\")\n",
    "unlabeled = chatData.iloc[:,1].values\n",
    "# TODO:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.2: Linear regression\n",
    "\n",
    "Linear regression is a technique that tries to find a correlation between a set of input variables x and a dependant variable y. In mathematical terms:\n",
    "\n",
    "$$y = \\alpha + \\beta X + \\epsilon$$\n",
    "\n",
    "where:\n",
    "- $X$ is the predictive vector, containing the (predictive) variables\n",
    "- $\\alpha$ and $\\beta$ are the model's parameters, where $\\alpha$ is the intercept/bias, $\\beta$ the coefficient vector containing coefficients for each predictive variable\n",
    "- and $\\epsilon$ the prediction error.\n",
    "\n",
    "Note that the assumption made is that the relationship is linear. This is a special case of polynomial regression, where we would allow for e.g. squared relationships.\n",
    "\n",
    "Our dependant variable is the negativity in the chat. Therefore we need to convert our labels into numbers first: We will use 0 for neutral, -1 for negative and +1 for positive sentiments. This is, of course, a simplification, as not all negative statements are equally negative. But we need to acknowledge that it is simply impossible to make an accurate distinction without knowing any context. And if we knew that, there would be no point in doing this regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the labels into values\n",
    "sentiments = []\n",
    "for i in chat_labels:\n",
    "    if i == 'positive':\n",
    "        sentiments.append(1)\n",
    "    elif i == 'negative':\n",
    "        sentiments.append(-1)\n",
    "    else:\n",
    "        sentiments.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in predictions:\n",
    "    if i == 'positive':\n",
    "        sentiments.append(1)\n",
    "    elif i == 'negative':\n",
    "        sentiments.append(-1)\n",
    "    elif i == 'neutral':\n",
    "        sentiments.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Preparation:\n",
    "\n",
    "1. First, we need to read the csv files and group them by `match_id`.\n",
    "\n",
    "2. We will create a dataframe containing all relevant information and each row will represent one match. An empty dataframe has already been created for you with all the columns you need to fill. Here is an explanation of what you need to put into each column:\n",
    "\n",
    "3. Create a list of tuples called `full_chatdata`, each tuple has the following structure: label, team.\n",
    "\n",
    "4. Create a list called `goldData` containing the gold advantage for every timestamp of a match (usually every minute).\n",
    "\n",
    "5. Create a list called `KDratios` of kill-death ratios for each player in a match, split into two parts, one for each team, called ratiosRadiant and ratiosDire.\n",
    "\n",
    "6. Add an additional column called `radiant_win` displaying the winning team using boolean.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "- you can find the labels in the chatData dataframe\n",
    "- use the `slot` column to determine the team. 0 to 4 is for radiant, 5-9 is for dire\n",
    "- There is a column in the match.csv file called `radiant_win` that displays true if team radiant won, false if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Read the csv files and group them by match id\n",
    "chatData = pd.read_csv(\"chat.csv\")\n",
    "chatData = chatData.drop(['unit'],axis=1)\n",
    "chatData['label'] = sentiments # We are assigning labels to the chat messages\n",
    "chatData = chatData.groupby('match_id')\n",
    "player_times = pd.read_csv(\"player_time.csv\")\n",
    "player_times = player_times.groupby('match_id')\n",
    "match_info = pd.read_csv(\"match.csv\")\n",
    "radiant_win = match_info['radiant_win']\n",
    "player_info = pd.read_csv(\"players.csv\")\n",
    "player_info = player_info[['match_id', 'kills','deaths']]\n",
    "player_info = player_info.groupby('match_id')\n",
    "\n",
    "\n",
    "# 2. Create the dataframe\n",
    "dataframe = pd.DataFrame(columns = ['chatData','goldDataAvg', 'KDratios', 'radiant_win'])\n",
    "\n",
    "# 3.\n",
    "full_chatdata = []\n",
    "\n",
    "for name, group in chatData:\n",
    "    chat_data_line = []\n",
    "    for index,row in group.iterrows():\n",
    "        chat_tuple = []\n",
    "        # TODO: Create a list of tuples called full_chatdata, each tuple has the following structure: label, team.\n",
    "        # Hint 1: use the label column to determine the negativity/positivity of the message\n",
    "        # Hint 2: use the 'slot' column to determine the team. 0 to 4 is for radiant, 5-9 is for dire.\n",
    "        \n",
    "        \n",
    "# 4. Create a list containing the gold advantage\n",
    "full_golddata =[]\n",
    "\n",
    "for name,group in player_times:\n",
    "    radiantAdv =[]\n",
    "    for index, row in group.iterrows():\n",
    "        radiantAdv.append((row['gold_t_0']+row['gold_t_1']+row['gold_t_2']+row['gold_t_3']+row['gold_t_4'])-\n",
    "            (row['gold_t_128']+row['gold_t_129']+row['gold_t_130']+row['gold_t_131']+row['gold_t_132']))\n",
    "        \n",
    "    full_golddata.append(radiantAdv)\n",
    "\n",
    "\n",
    "# 5.\n",
    "full_playerinfo = []\n",
    "for name, group in player_info:\n",
    "    playerinfo = []\n",
    "    for index, row in group.iterrows():\n",
    "        killsdeaths = []\n",
    "        killsdeaths.append(row['kills'])\n",
    "        killsdeaths.append(row['deaths'])\n",
    "        playerinfo.append(killsdeaths)\n",
    "    full_playerinfo.append(playerinfo)\n",
    "\n",
    "full_KDRatios = []\n",
    "\n",
    "for row in full_playerinfo:\n",
    "    KDRatios = []\n",
    "    ratiosRadiant =[]\n",
    "    ratiosDire = []\n",
    "    for i,player in enumerate(row):\n",
    "        # TODO: Create a list called [...] kill-death ratios for each player\n",
    "        # Hint: For each game the kd ratios should look like the following: \n",
    "        # [[RadiantPlayer0KD, ... RadiantPlayer4KD],[DirePlayer0KD, ... DirePlayer4KD]] \n",
    "        \n",
    "        \n",
    "# We add the newly created columns to our dataframe       \n",
    "dataframe['chatData'] = full_chatdata\n",
    "dataframe['goldData'] = full_golddata\n",
    "dataframe['radiant_win'] = radiant_win\n",
    "dataframe['KDratios'] = full_KDRatios\n",
    "        \n",
    "dataframe.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Per-match analysis:\n",
    "\n",
    "As you may have noticed, the gold variables are gathered every minute, but the chat times are irregular. We could try to group the chat into 60 second timeframes that would correspond with the gold values, but this would be too tedious. Instead, we will simplify this by looking at the game as a whole:\n",
    "\n",
    "1. Compute the average negativity for each team by iterating over the list of tuples you created in exercise 4.2.3.\n",
    "\n",
    "2. Then, compute the average gold advantage for each match, and add a column for the gold advantage at the end of a match. The gold advantage at the end of a match is the last value of the list.\n",
    "\n",
    "3. Create a new column for the difference in negativity between the two teams.\n",
    "\n",
    "4. The kill/death ratios aren't very useful in the current format. Take the lowest K/D ratio from each team and create new columns for them. The reasoning behind this is that a low K/D ratio is a sign of underperformance of a player and players who do not perform on an acceptable level are usually harassed more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import  svm \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "# 1. Average negativity\n",
    "radiantToxicity_full = []\n",
    "direToxicity_full = []\n",
    "\n",
    "for index, row in dataframe.iterrows():\n",
    "    radiantToxicity = 0\n",
    "    direToxicity = 0\n",
    "    # These counters keep of track of the number of messages each team wrote:\n",
    "    radiantcounter = 0\n",
    "    direcounter = 0\n",
    "    for tuples in row['chatData']:\n",
    "        # TODO: Calculate each team's toxicity by summing all labels of a match.\n",
    "        # Hint: Don't forget to keep count of the number of messages written by each team.\n",
    "        \n",
    "        \n",
    "# 2. Average gold\n",
    "goldAverages = []\n",
    "goldEnd = []\n",
    "for index, row in dataframe.iterrows():\n",
    "    # TODO: Compute the average gold advantage for each match, as well as the gold advantage at the end of the match.\n",
    "    # Hint: The column goldData contains a list with gold advantage per minutes.\n",
    "    \n",
    "    \n",
    "# 3. Difference in negativity\n",
    "differences = []\n",
    "# TODO: Compute the difference in negativity between the 2 teams.\n",
    "\n",
    "\n",
    "# 4. K/D ratios\n",
    "worstRadiant = []\n",
    "worstDire = []\n",
    "for index, row in dataframe.iterrows():\n",
    "    # TODO: Take the lowest K/D ratio from each team and create new columns for them.\n",
    "\n",
    "    \n",
    "# We add the newly created columns to our dataframe       \n",
    "dataframe['toxicityR'] = radiantToxicity_full\n",
    "dataframe['toxicityD'] = direToxicity_full\n",
    "dataframe['goldData'] = goldAverages\n",
    "dataframe['goldEnd'] = goldEnd \n",
    "dataframe['diff'] = differences\n",
    "dataframe['worstKDR'] = worstRadiant\n",
    "dataframe['worstKDD'] = worstDire\n",
    "\n",
    "dataframe.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) A warm-up regression\n",
    "\n",
    "Before we test our hypothesis of whether or not the state of the game influences player behavior, we will perform a linear regression with only one input variable: The gold advantage. \n",
    "\n",
    "You have probably wondered why we just assume that the gold values would represent the state of a game, whether a team is losing or winning. So far, this has only been a theory, and we should test it, as it would not make sense to use it as a representation for the state of the game in the actual regression model, if it wasn't representative at all.\n",
    "\n",
    "1. Once again, split your data into a train set and test set, create a linear regression model, fit the data and print your score. Try it two times: Your dependant variable should always be `radiant_win`, your X should be the average gold advantage and the gold advantage at the end. \n",
    "\n",
    "2. Discuss the score you obtained! What do the results mean for the explanatory power of the gold variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# TODO 1:\n",
    "X = # TODO \n",
    "y = # TODO\n",
    "\n",
    "# Splitting the data into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25) \n",
    "  \n",
    "regr = LinearRegression() \n",
    "  \n",
    "regr.fit(X_train, y_train) \n",
    "print(regr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = regr.predict(X_test) \n",
    "plt.scatter(X_test, y_test, color ='b') \n",
    "plt.plot(X_test, y_pred, color ='k') \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Write your observations here:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Testing our hypothesis\n",
    "\n",
    "**1.** Finally, we can do our linear regression. This time, use the gold data, the kill/death ratios and the negativity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO 1:\n",
    "X = # TODO \n",
    "Y = # TODO\n",
    " \n",
    "# with sklearn\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, Y)\n",
    "print(regr.score(X,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Discussion\n",
    "\n",
    "What is the score? What does that number mean? Discuss possible reasons for this result.  \n",
    "**Hint:** Take a peek at the labels.xlsx file and look at some of the most common negative words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Write your observations here:**\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
